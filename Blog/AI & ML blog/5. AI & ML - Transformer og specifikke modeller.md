 

##  Mål for ugen
Yderligere forståelse af transformer modeller og deres egenskaber, samt lidt læring om de specifikke modeller. 
Derudover lidt forståelse af termerne chatbot og agent. 
Opstart af lille projekt i python for at afprøve huggingface og python i det hele taget.

---

##  Hvad jeg lærte
Det gik op for mig i denne uge at der er 3 typer af transformer-modeller.
Da der udover encoder og decoder er en sammensat version, der hedder Encoder-decoder.
At BERT-modellen er en oplagt model at bruge til sundhedsdata i forhold til forståelse og nedbrydning af tekster af den slags karakter
[[LLM og transformers]]
[[Chatbot i Python test 1. ( Setup & in memory vector søgning)]]
---


---

##  Refleksion

Der er en del nyt at vende sig til i python. Men da det ikke er mit emne i dette semester og da der er begrænset tid, så må det blive en hurtig forstålse af python, samt en "Learning by doing" og ved hjælp af AI vibe-coding jeg vil indlede mine python forsøg.


 Projekt specifikt.
Angående behandling af sundhedsdata, så Er BERT-Encoder modellen interessant til vores domæne som den primære model der vil stå for behnadling af sundhedsdata og tekster. 
Dette er dog ikke mit område.
Til min opgave at lave en "side chatbot" der er det vigtigere med en nem og simpel model der kan køre gnidningsfrit på siden og mange modeller skulle kunne løse denne opgave helt fint.
Det vil dog være et oplagt valg til de to andre chatbots i vores projekt.