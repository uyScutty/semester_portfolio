
# **Uddybning til Dagbog Etape 2 (LLMâ€™er & Transformers)**

_(Dette er den version du kan linke til fra din dagbog.)_

---

## ğŸ§  **Hvad er en LLM egentlig?**

En Large Language Model (LLM) er en maskinlÃ¦ringsmodel, der er trÃ¦net til at forstÃ¥ og generere menneskeligt sprog.  
Den arbejder ikke med â€œordâ€ som mennesker, men bruger matematiske reprÃ¦sentationer, der gÃ¸r det muligt at:

- forstÃ¥ mening
    
- finde sammenhÃ¦nge
    
- svare, opsummere, forklare, analysere
    
- forudsige nÃ¦ste token i en sÃ¦tning
    

LLMâ€™er er en slags â€œtekstmotorâ€ bygget ovenpÃ¥ transformer-arkitekturen.

---

## ğŸ§© **Transformers â€“ den underliggende arkitektur**

Transformers er den teknologi som moderne LLMâ€™er (GPT, DeepSeek, Claude, Llama, Gemma osv.) bygger pÃ¥.  
Tre ting gÃ¸r transformers specielle:

---

### **1) Attention (fokusmekanismen)**

Attention gÃ¸r det muligt for modellen at:

- lÃ¦se hele sÃ¦tningen pÃ¥ Ã©n gang
    
- forstÃ¥ relationer mellem ord
    
- fokusere pÃ¥ vigtige detaljer
    
- holde styr pÃ¥ kontekst
    

Eksempel:  
I sÃ¦tningen _â€œpatienten tog medicinen fordi hun fÃ¸lte svimmelhedâ€_  
skal modellen forstÃ¥ at â€œhunâ€ refererer til â€œpatientenâ€ og at â€œsvimmelhedâ€ er Ã¥rsagen.

Det krÃ¦ver **attention**.

---

### **2) Parallel lÃ¦sning**

Ã†ldre modeller (RNN, LSTM) lÃ¦ste tekst Ã©t ord ad gangen.  
Transformers lÃ¦ser _alt samtidigt_, hvilket giver:

- hurtigere modeller
    
- bedre forstÃ¥else af lange tekster
    
- bedre logiske svar
    

---

### **3) Lag pÃ¥ lag af forstÃ¥else**

Transformers bestÃ¥r af mange lag, og hvert lag lÃ¦rer noget forskelligt:

- nogle lag lÃ¦rer grammatik
    
- nogle lÃ¦rer fakta
    
- nogle lÃ¦rer logik
    
- nogle lÃ¦rer sammenhÃ¦nge mellem begreber
    

Det gÃ¸r dem fleksible og anvendelige pÃ¥ mange omrÃ¥der â€” ogsÃ¥ sundhedsdata.

---

## ğŸ§± **Tre vigtige arkitekturer: Encoder, Decoder og Encoderâ€“Decoder**

---

### **ğŸ”¹ Encoder-only modeller (f.eks. BERT)**

Disse modeller er rigtig gode til:

- at forstÃ¥ tekst
    
- klassifikation
    
- sentiment
    
- sÃ¸gning
    
- embeddings til RAG
    

De genererer **ikke tekst**.

â†’ I dit projekt bruges encoder-lignende funktionalitet indirekte gennem _embeddings_ i RAG.

---

### **ğŸ”¹ Decoder-only modeller (f.eks. GPT, Claude, DeepSeek)**

De kan:

- generere tekst
    
- fÃ¸re samtaler
    
- svare pÃ¥ spÃ¸rgsmÃ¥l
    
- lÃ¸se opgaver
    
- forklare komplekse ting
    

De er de klassiske chatbot-modeller.

â†’ I dit projekt er dette den type du bruger til din **guider-agent** og sundhedsagent.

---

### **ğŸ”¹ Encoderâ€“decoder modeller (f.eks. T5, FLAN-T5)**

De er bedst til:

- oversÃ¦ttelse
    
- opsummering
    
- konvertering af tekst fra Ã©n form til en anden
    

â†’ Ikke lige sÃ¥ relevante for selve chatbotdelen, men nyttige til databehandling.

---

## ğŸŒ Hvorfor modellernes forskelle betyder noget i praksis

Du nÃ¦vner i din dagbog, at modeller opfÃ¸rer sig forskelligt.

Her er **hvorfor**:

### ğŸ”¸ De er trÃ¦net pÃ¥ forskellige datasÃ¦t

Nogle har meget sundhedsdata.  
Nogle har nÃ¦sten intet.  
Nogle modeller â€œforstÃ¥râ€ bedre dansk eller engelsk end andre.

### ğŸ”¸ De bruger forskellige tokenizere

Danske eller medicinske ord kan vÃ¦re â€œsvÃ¦reâ€ for visse modeller.

Eksempel:  
â€œkronisk sygdomâ€ kan blive 3 tokens i Ã©n model og 6 tokens i en anden.

â†’ PÃ¥virker pris og kvalitet.

### ğŸ”¸ De har forskellige styrker

- OpenAI â†’ stÃ¦rk til reasoning, forklaringer
    
- Claude â†’ stÃ¦rk til lange dokumenter
    
- DeepSeek â†’ stÃ¦rk til logik og kode
    
- Mistral/Llama â†’ stÃ¦rke lokale modeller
    
- BERT â†’ stÃ¦rk til forstÃ¥else, ikke generering
    

â†’ Derfor skal du vÃ¦lge model efter agentens opgave.

---

## ğŸ¥ Relevans for dit sundhedsprojekt

Du nÃ¦vner i blog 2, at dine chatagenter skal forstÃ¥ sundhedsdata.

Det stiller krav:

### âœ”ï¸ Modellen skal forstÃ¥ fagtermer

Som â€œakut svimmelhedâ€, â€œbeta-histidinâ€, â€œblodtrykâ€, â€œmigraine auraâ€, osv.

### âœ”ï¸ Modellen skal vÃ¦re god til kontekstforstÃ¥else

F.eks. nÃ¥r brugerens sÃ¦tning indeholder symptomer, Ã¥rsager og kontekst.

### âœ”ï¸ Modellen mÃ¥ ikke â€œhallucinereâ€ for meget

Derfor er _modelforstÃ¥else_ og transformer-kendskab vigtigt.

### âœ”ï¸ Derfor skal du teste forskellige LLMâ€™er

â€” isÃ¦r dem med god trÃ¦ning pÃ¥ sundhedsrelateret indhold.