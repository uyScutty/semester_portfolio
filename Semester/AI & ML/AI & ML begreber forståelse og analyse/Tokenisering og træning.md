# **LÃ¦ringsside â€“ Uddybning til Etape 3 (Tokenisering & ModeltrÃ¦ning)**

Denne side uddyber de begreber du kort nÃ¦vner i blogindlÃ¦gget. Den kan bruges som â€œekstra forklaringâ€ til dig selv eller som bilag, hvis nogen skal forstÃ¥ lÃ¦ringen bag.

---

## ğŸ§© **Hvad er tokenisering â€“ og hvorfor er det vigtigt?**

Tokenisering er processen hvor tekst bliver opdelt i mindre enheder, der kaldes _tokens_.  
Modellerne arbejder ikke med ord som vi kender dem, men med talrÃ¦kker som reprÃ¦senterer tokens.

Eksempel:  
Ordet _â€sunhedstilstandâ€_ kan blive delt i flere tokens:

- sund
    
- hed
    
- stil
    
- stand
    

Dette afhÃ¦nger af den algoritme modellen bruger.

Tokenisering er vigtig fordi:

- den bestemmer **hvordan modellen forstÃ¥r tekst**
    
- den pÃ¥virker **pris** (flere tokens = dyrere)
    
- den pÃ¥virker **kvalitet** (fagtermer â†’ bedre eller dÃ¥rligere delt)
    
- den pÃ¥virker **prÃ¦cision**, isÃ¦r i sundhedsdata
    

---

## ğŸ”§ **Byte Pair Encoding (BPE)**

En af de mest brugte algoritmer.

Princip:  
Start med helt smÃ¥ enheder (bogstaver), og slÃ¥ de kombinationer sammen der forekommer oftest.

Fordele:

- effektiv til mange sprog
    
- hÃ¥ndterer nye ord godt
    
- bruges i GPT, Claude, DeepSeek, Llama
    

Ulemper:

- faglige ord kan blive delt meget, hvilket gÃ¸r dem svÃ¦rere for modellen
    

---

## ğŸ”§ **WordPiece**

Anvendes bl.a. af BERT.

Princip:  
Finder subwords baseret pÃ¥ sandsynlighed i trÃ¦ningsdata.

Fordele:

- godt til generelle domÃ¦ner
    
- stabil struktur
    

Ulemper:

- medicinske eller sjÃ¦ldne ord splittes ofte ekstremt meget
    

---

## ğŸ”§ **SentencePiece**

En anden tokeniseringsmetode, ofte brugt i open-source modeller.

Fordele:

- kan arbejde uden mellemrum (â€œwhitespace-agnosticâ€)
    
- god til ikke-engelske sprog
    

---

## ğŸ§  **Hvorfor bruger LLMâ€™er subwords?**

Modellerne kan ikke lÃ¦re alle ord i alle sprog.  
Subwords er en lÃ¸sning:

- modellen lÃ¦rer stykker af ord
    
- kan kombinere dem til nye ord
    
- bedre generalisering
    
- fÃ¦rre tokens end bogstav-for-bogstav modeller
    

For sundhedsdomÃ¦net betyder det:

- nogle modeller forstÃ¥r medicinske ord bedre end andre
    
- ord som â€œbetahistineâ€ eller â€œlabyrintitisâ€ kan vÃ¦re Ã©t token eller fem
    
- dette pÃ¥virker forstÃ¥elsen i en chatbot
    

---

## ğŸ”¬ **Hvordan pÃ¥virker tokenisering modeltrÃ¦ning?**

NÃ¥r en model trÃ¦nes, lÃ¦rer den mÃ¸nstre baseret pÃ¥ tokens, ikke ord.

Hvis et vigtigt ord bliver delt op i mange subwords, sÃ¥ skal modellen:

- lÃ¦re mÃ¸nstre mellem flere tokens
    
- bruge mere kontekst
    
- bruge flere ressourcer
    
- har stÃ¸rre risiko for at misforstÃ¥ sammenhÃ¦nge
    

â†’ Derfor er nogle modeller bedre til sundhedsdata end andre.

---

## ğŸ’¸ **Tokenisering og pris**

Hvis du bruger API-modeller (OpenAI, Claude osv.):

- pris beregnes per token
    
- flere subwords = dyrere
    
- lange ord â†’ dyre forkerte modeller
    

OpenAI og Claude kan vÃ¦re dyrere, fordi deres tokenizers splitter dansk + medicinsk tekst mere aggressivt.

---

## ğŸ†š **Hvorfor forskellige modeller giver forskellige resultater**

Din blog nÃ¦vner forskelle mellem modeller.

Tokenisering spiller en rolle fordi:

- modeller er trÃ¦net pÃ¥ forskellige typer data
    
- deres tokenizere er optimeret til forskellige sprog
    
- subword-logikken pÃ¥virker hvordan de â€œforstÃ¥râ€ fagtermer
    
- encoder-modeller og decoder-modeller bruger nogle gange forskellige tokenizers
    

Dette forklarer hvorfor:

- nogle modeller er bedre til dansk
    
- nogle er bedre til medicinske termer
    
- nogle er bedre til komplekse brugerbeskeder
    
- prisen Ã¦ndrer sig fra model til model
    

---

## ğŸ©º **Relevans for dit sundhedsprojekt**

NÃ¥r du bygger chatagenter der skal hÃ¥ndtere sundhedsindhold, skal du vÃ¦re opmÃ¦rksom pÃ¥:

1. **Hvilken tokenizer modellen bruger**  
    â†’ pÃ¥virker forstÃ¥else af medicinske ord.
    
2. **Hvor mange tokens der bruges**  
    â†’ pÃ¥virker pris og begrÃ¦nsninger i kontekst.
    
3. **ModeltrÃ¦ningen**  
    â†’ nogle modeller har set mere sundhedsdata end andre.
    
4. **Dokumenternes lÃ¦ngde**  
    â†’ vigtig ved RAG, hvor tekst chunkes efter tokens.
    

Alt dette har betydning for prÃ¦cision og kvalitet i dine chatagenter.

---

# âœ”ï¸ Denne lÃ¦ringsside matcher direkte til:

**â€œMÃ¥l for etapenâ€**  
â†’ du Ã¸nskede at gÃ¥ dybere i forstÃ¥elsen af tokenisering og modeltrÃ¦ning.

**â€œHvad jeg lÃ¦rteâ€**  
â†’ du nÃ¦vnte BPE, WordPiece og subwords â€” her er uddybningen af det.

**â€œRefleksionerâ€**  
â†’ forskelle mellem modeller, forstÃ¥else af sundhedsdata, og hvad det betyder i praksis.

# ğŸ§© **Hvordan laver den chunks? (kort svar)**

Den tager **lange tekster** og deler dem op i **mindre stykker**, typisk:

- 300â€“500 ord
    
- klippet ved punktummer
    
- aldrig tilfÃ¦ldigt
    
- men det er simpel splitting
    

FormÃ¥let:  
âœ” gÃ¸re teksterne nemmere at embedde  
âœ” gÃ¸re sÃ¸gning mere prÃ¦cis  
âœ” undgÃ¥ for store embeddings

---

# ğŸ§© **Hvad er en embedding? (super kort)**

En _embedding_ er:

â¡ï¸ **en liste af tal som reprÃ¦senterer meningen i teksten**  
fx:

`[0.12, -0.55, 0.87, ...]  (384 tal)`

Det er IKKE ord  
Det er IKKE tokens  
Det er betydning.

---

# ğŸ§© **Hvad er en vektor? (endnu kortere)**

En embedding _er_ en vektor.

SÃ¥:

- **Embedding** = betydningsvector
    
- **Vector** = talrÃ¦kke der kan bruges i matematisk sÃ¸gning
    

FAISS bruger disse vektorer til:

â¡ï¸ finde de tekststykker som ligner mest din query.