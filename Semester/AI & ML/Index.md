
# AI og ML i projektet â€“ hvad bruger vi, og hvordan?

I projektet arbejder vi bÃ¥de med **AI** og **ML**, men de spiller meget forskellige roller.  
Den korte realitet er:

- **AI stÃ¥r for langt stÃ¸rstedelen af funktionaliteten**
    
- **ML bruges primÃ¦rt bag kulissen til nogle fÃ¥ specifikke teknikker**
    

Det er vigtigt at skelne mellem **at trÃ¦ne modeller** (klassisk maskinlÃ¦ring)  
og **at bruge modeller** (AI-inference).  
I vores projekt gÃ¸r vi det sidste.

---

# ğŸ”· AI â€“ den centrale del af systemet

Langt hovedparten af projektets â€œintelligensâ€ kommer fra en **stor sprogmodel (LLM)**, som vi bruger til at:

- forstÃ¥ brugerens spÃ¸rgsmÃ¥l
    
- fÃ¸lge en bestemt tone og personlighed
    
- generere naturlige og relevante svar
    
- kombinere spÃ¸rgsmÃ¥l med kontekstdokumenter (RAG)
    
- hÃ¥ndhÃ¦ve regler som â€œingen medicinske diagnoserâ€ eller â€œbrug kun dokumenteret kontekstâ€
    

I praksis betyder det, at stort set hele chatbot-oplevelsen â€”  
det sproglige, det forstÃ¥ende, det forklarende â€”  
stammer fra AI-modellens evner.

### AI bruges isÃ¦r i:

- prompt-builderen (tone, stil, regler)
    
- selve chatfunktionaliteten
    
- RAG-svarene
    
- Python APIâ€™et der hÃ¥ndterer forespÃ¸rgsler
    
- den mÃ¥de modellen forbinder kontekst og spÃ¸rgsmÃ¥l
    

AI-modellen er altsÃ¥ motoren, der producerer svarene og styrer hele samtalen.

---

# ğŸ”¶ ML â€“ en mindre, men vigtig teknisk komponent

Selvom projektet ikke trÃ¦ner egne modeller, bruger vi stadig nogle **maskinlÃ¦ringsteknikker** i baggrunden.

Der er isÃ¦r tre steder, hvor ML er i spil:

### 1ï¸âƒ£ Embeddings

Teksten fra vores dokumenter omdannes til **vektorer** via en embedding-model (som selv er trÃ¦net gennem ML).  
De vektorer bruges til at mÃ¥le betydning og lighed mellem tekststykker.

### 2ï¸âƒ£ Vektor-sÃ¸gning i Chroma

NÃ¥r chatbotten skal finde relevant kontekst, laver Chroma:

- nearest neighbor search
    
- cosine similarity
    
- rangering af de mest relevante dokumenter
    

Det er ML-inspirerede teknikker, men vi trÃ¦ner ikke noget selv.

### 3ï¸âƒ£ Tokenisering

Al brug af LLMâ€™er involverer en ML-trÃ¦net tokeniseringsmodel, som opdeler tekst i tokens.  
Det sker automatisk og er en del af pipelines.

---

# ğŸ§© Hvordan AI og ML spiller sammen

Man kan opsummere det sÃ¥dan her:

- **ML hjÃ¦lper systemet med at finde den rigtige viden**  
    (embeddings, vector search)
    
- **AI bruger den viden til at skrive et meningsfuldt svar**  
    (sprogforstÃ¥else, tone, reasoning)
    

Det er altsÃ¥ en samarbejdsmodel mellem to teknologier, hvor AIâ€™en stÃ¥r for det meste af brugeroplevelsen, og ML hjÃ¦lper med strukturering og hentning af viden bag scenen.

---

# ğŸ“Š Fordelingen i praksis

Selv om man ofte taler om AI og ML som to lige store dele, ser det anderledes ud i virkelige projekter som vores:

- **AI (â‰ˆ 98%)**  
    Hele sprogforstÃ¥elsen og genereringen kommer fra en LLM, som vi ikke selv trÃ¦ner.
    
- **ML (â‰ˆ 2%)**  
    Bruges til embeddings, tokens og vector search â€“ teknikker vi anvender, men ikke udvikler.
    

Denne fordeling er typisk i moderne RAG-systemer, hvor man bygger ovenpÃ¥ fÃ¦rdigtrÃ¦nede modeller.





[[Ai & ML/LÃ¦ringsmÃ¥l]]